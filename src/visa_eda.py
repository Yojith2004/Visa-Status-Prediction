# -*- coding: utf-8 -*-
"""visa_EDA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13bWvT90kJ2AeQNlRMesgxLakM0jNrFeK
"""

# MODULE 2: Exploratory Data Analysis (EDA)
# AI Enabled Visa Status Prediction and Processing Time Estimator

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

# Set visualization style
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

# Load preprocessed data
file_path = '/content/drive/MyDrive/visa_data_preprocessed.csv'
df = pd.read_csv(file_path, parse_dates=['RECEIVED_DATE', 'DECISION_DATE'])

print(f"Dataset loaded: {df.shape[0]} records, {df.shape[1]} columns")
print(f"Date range: {df['RECEIVED_DATE'].min().date()} to {df['RECEIVED_DATE'].max().date()}")


print("\n" + "="*60)
print("1. BASIC STATISTICAL ANALYSIS")
print("="*60)

# Target variable analysis
print("\nProcessing Days Statistics:")
print(df['processing_days'].describe())
print(f"\nSkewness: {df['processing_days'].skew():.3f}")
print(f"Kurtosis: {df['processing_days'].kurtosis():.3f}")

# Categorical variable analysis
print("\nTop 5 values for key categorical columns:")
key_categorical = ['CASE_STATUS', 'VISA_CLASS', 'EMPLOYER_STATE', 'WORKSITE_STATE', 'FULL_TIME_POSITION']
for col in key_categorical:
    if col in df.columns:
        print(f"\n{col}:")
        print(df[col].value_counts().head())


print("\n" + "="*60)
print("2. PROCESSING TIME DISTRIBUTION ANALYSIS")
print("="*60)

fig1, axes1 = plt.subplots(2, 3, figsize=(18, 10))

# Overall distribution
axes1[0, 0].hist(df['processing_days'], bins=100, edgecolor='black', alpha=0.7)
axes1[0, 0].axvline(df['processing_days'].median(), color='red', linestyle='--',
                   label=f'Median: {df["processing_days"].median():.1f} days')
axes1[0, 0].set_xlabel('Processing Days')
axes1[0, 0].set_ylabel('Frequency')
axes1[0, 0].set_title('Overall Processing Time Distribution')
axes1[0, 0].legend()
axes1[0, 0].grid(True, alpha=0.3)

# Box plot of processing days
axes1[0, 1].boxplot(df['processing_days'])
axes1[0, 1].set_ylabel('Processing Days')
axes1[0, 1].set_title('Processing Time Box Plot')
axes1[0, 1].grid(True, alpha=0.3)

# Log transformation for better visualization
log_processing = np.log1p(df['processing_days'])
axes1[0, 2].hist(log_processing, bins=50, edgecolor='black', alpha=0.7)
axes1[0, 2].set_xlabel('Log(Processing Days + 1)')
axes1[0, 2].set_ylabel('Frequency')
axes1[0, 2].set_title('Log-Transformed Processing Time')
axes1[0, 2].grid(True, alpha=0.3)

# Processing time by CASE_STATUS
if 'CASE_STATUS' in df.columns:
    status_processing = df.groupby('CASE_STATUS')['processing_days'].agg(['mean', 'median', 'count']).sort_values('mean')
    status_processing.plot(kind='bar', y='mean', ax=axes1[1, 0], legend=False)
    axes1[1, 0].set_xlabel('Case Status')
    axes1[1, 0].set_ylabel('Average Processing Days')
    axes1[1, 0].set_title('Average Processing Time by Case Status')
    axes1[1, 0].tick_params(axis='x', rotation=45)

# Processing time by VISA_CLASS (Top 10)
if 'VISA_CLASS' in df.columns:
    visa_stats = df.groupby('VISA_CLASS')['processing_days'].agg(['mean', 'count']).sort_values('count', ascending=False).head(10)
    visa_stats['mean'].plot(kind='bar', ax=axes1[1, 1])
    axes1[1, 1].set_xlabel('Visa Class')
    axes1[1, 1].set_ylabel('Average Processing Days')
    axes1[1, 1].set_title('Top 10 Visa Classes by Average Processing Time')
    axes1[1, 1].tick_params(axis='x', rotation=45)

# Processing time by FULL_TIME_POSITION
if 'FULL_TIME_POSITION' in df.columns:
    full_time_stats = df.groupby('FULL_TIME_POSITION')['processing_days'].agg(['mean', 'median', 'count'])
    full_time_stats['mean'].plot(kind='bar', ax=axes1[1, 2])
    axes1[1, 2].set_xlabel('Full Time Position')
    axes1[1, 2].set_ylabel('Average Processing Days')
    axes1[1, 2].set_title('Processing Time by Employment Type')
    axes1[1, 2].tick_params(axis='x', rotation=0)

plt.tight_layout()
plt.show()


print("\n" + "="*60)
print("3. REGIONAL ANALYSIS")
print("="*60)

fig2, axes2 = plt.subplots(2, 2, figsize=(16, 12))

# Analysis by EMPLOYER_STATE (Top 15)
if 'EMPLOYER_STATE' in df.columns:
    employer_state_stats = df.groupby('EMPLOYER_STATE')['processing_days'].agg(['mean', 'count']).sort_values('count', ascending=False).head(15)

    # Bar chart for average processing time
    axes2[0, 0].bar(employer_state_stats.index, employer_state_stats['mean'])
    axes2[0, 0].set_xlabel('Employer State')
    axes2[0, 0].set_ylabel('Average Processing Days')
    axes2[0, 0].set_title('Top 15 States by Average Processing Time (Employer)')
    axes2[0, 0].tick_params(axis='x', rotation=45)

    # Scatter plot: Volume vs Processing Time
    axes2[0, 1].scatter(employer_state_stats['count'], employer_state_stats['mean'], alpha=0.6)
    axes2[0, 1].set_xlabel('Number of Applications')
    axes2[0, 1].set_ylabel('Average Processing Days')
    axes2[0, 1].set_title('Volume vs Processing Time by State')

    # Add state labels
    for idx, row in employer_state_stats.iterrows():
        axes2[0, 1].annotate(idx, (row['count'], row['mean']), fontsize=8)

# Analysis by WORKSITE_STATE (Top 15)
if 'WORKSITE_STATE' in df.columns:
    worksite_state_stats = df.groupby('WORKSITE_STATE')['processing_days'].agg(['mean', 'count']).sort_values('count', ascending=False).head(15)

    # Heatmap style visualization
    states = worksite_state_stats.index
    means = worksite_state_stats['mean'].values

    # Create gradient color based on processing time
    colors = plt.cm.RdYlGn_r((means - means.min()) / (means.max() - means.min()))
    bars = axes2[1, 0].barh(range(len(states)), means, color=colors)
    axes2[1, 0].set_yticks(range(len(states)))
    axes2[1, 0].set_yticklabels(states)
    axes2[1, 0].set_xlabel('Average Processing Days')
    axes2[1, 0].set_title('Processing Time Heatmap by Worksite State')
    axes2[1, 0].invert_yaxis()

    # Add value labels
    for bar, mean_val in zip(bars, means):
        axes2[1, 0].text(bar.get_width() + 1, bar.get_y() + bar.get_height()/2,
                        f'{mean_val:.1f}', va='center', fontsize=9)

# Correlation between employer and worksite states
if all(col in df.columns for col in ['EMPLOYER_STATE', 'WORKSITE_STATE', 'processing_days']):
    # Sample for better visualization
    sample_df = df.sample(min(1000, len(df)), random_state=42)
    scatter = axes2[1, 1].scatter(range(len(sample_df)), sample_df['processing_days'],
                                 c=sample_df['EMPLOYER_STATE'].astype('category').cat.codes,
                                 alpha=0.6, cmap='tab20', s=30)
    axes2[1, 1].set_xlabel('Application Index')
    axes2[1, 1].set_ylabel('Processing Days')
    axes2[1, 1].set_title('Processing Time by State (Color-coded)')
    axes2[1, 1].set_yscale('log')

    # Add a simple legend for top 5 states
    top_states = df['EMPLOYER_STATE'].value_counts().head(5).index
    legend_elements = []
    for state in top_states:
        color = scatter.cmap(scatter.norm(df[df['EMPLOYER_STATE']==state].index[0] % 20))
        legend_elements.append(plt.Line2D([0], [0], marker='o', color='w',
                                         markerfacecolor=color, markersize=8, label=state))
    axes2[1, 1].legend(handles=legend_elements, loc='upper right', fontsize=8)

plt.tight_layout()
plt.show()


print("\n" + "="*60)
print("4. SEASONAL TREND ANALYSIS")
print("="*60)

if 'application_month' in df.columns:
    # Monthly trends
    monthly_stats = df.groupby('application_month')['processing_days'].agg(['mean', 'median', 'count', 'std'])
    monthly_stats['cv'] = monthly_stats['std'] / monthly_stats['mean']  # Coefficient of variation

    # Seasonal trends
    if 'application_season' in df.columns:
        seasonal_stats = df.groupby('application_season')['processing_days'].agg(['mean', 'median', 'count', 'std']).reindex(['Winter', 'Spring', 'Summer', 'Fall'])

    fig3, axes3 = plt.subplots(2, 2, figsize=(16, 10))

    # Monthly average processing time
    axes3[0, 0].plot(monthly_stats.index, monthly_stats['mean'], marker='o', linewidth=2, markersize=8)
    axes3[0, 0].fill_between(monthly_stats.index,
                            monthly_stats['mean'] - monthly_stats['std'],
                            monthly_stats['mean'] + monthly_stats['std'],
                            alpha=0.2)
    axes3[0, 0].set_xlabel('Month')
    axes3[0, 0].set_ylabel('Average Processing Days')
    axes3[0, 0].set_title('Monthly Processing Time Trends with Std Dev')
    axes3[0, 0].set_xticks(range(1, 13))
    axes3[0, 0].grid(True, alpha=0.3)

    # Monthly application volume
    axes3[0, 1].bar(monthly_stats.index, monthly_stats['count'])
    axes3[0, 1].set_xlabel('Month')
    axes3[0, 1].set_ylabel('Number of Applications')
    axes3[0, 1].set_title('Monthly Application Volume')
    axes3[0, 1].set_xticks(range(1, 13))
    axes3[0, 1].grid(True, alpha=0.3)

    # Seasonal comparison
    if 'application_season' in df.columns:
        bars = axes3[1, 0].bar(seasonal_stats.index, seasonal_stats['mean'])
        axes3[1, 0].set_xlabel('Season')
        axes3[1, 0].set_ylabel('Average Processing Days')
        axes3[1, 0].set_title('Processing Time by Season')

        # Add value labels on bars
        for bar, mean_val in zip(bars, seasonal_stats['mean']):
            height = bar.get_height()
            axes3[1, 0].text(bar.get_x() + bar.get_width()/2., height + 1,
                            f'{mean_val:.1f}', ha='center', va='bottom', fontsize=10)

        # Heatmap: Month vs Year processing time
        if 'application_year' in df.columns:
            pivot_table = df.pivot_table(values='processing_days',
                                        index='application_year',
                                        columns='application_month',
                                        aggfunc='mean')

            im = axes3[1, 1].imshow(pivot_table, aspect='auto', cmap='YlOrRd')
            axes3[1, 1].set_xlabel('Month')
            axes3[1, 1].set_ylabel('Year')
            axes3[1, 1].set_title('Processing Time Heatmap: Year vs Month')
            axes3[1, 1].set_xticks(range(12))
            axes3[1, 1].set_xticklabels(range(1, 13))
            axes3[1, 1].set_yticks(range(len(pivot_table.index)))
            axes3[1, 1].set_yticklabels(pivot_table.index.astype(int))
            plt.colorbar(im, ax=axes3[1, 1], label='Average Processing Days')

    plt.tight_layout()
    plt.show()

    print("\nMonthly Statistics:")
    print(monthly_stats)

    if 'application_season' in df.columns:
        print("\nSeasonal Statistics:")
        print(seasonal_stats)


print("\n" + "="*60)
print("5. WORKLOAD ANALYSIS")
print("="*60)

# Analyze temporal workload patterns
if 'RECEIVED_DATE' in df.columns:
    # Create weekly and monthly workload analysis
    df['receive_week'] = df['RECEIVED_DATE'].dt.isocalendar().week
    df['receive_year'] = df['RECEIVED_DATE'].dt.year
    df['receive_year_week'] = df['receive_year'].astype(str) + '-W' + df['receive_week'].astype(str).str.zfill(2)

    # Weekly workload analysis
    weekly_workload = df.groupby('receive_year_week').agg({
        'CASE_NUMBER': 'count',
        'processing_days': 'mean'
    }).rename(columns={'CASE_NUMBER': 'application_count'})

    # Keep only weeks with sufficient data
    weekly_workload = weekly_workload[weekly_workload['application_count'] > 10]

    fig4, axes4 = plt.subplots(2, 2, figsize=(16, 10))

    # Weekly application volume (top 30 weeks for clarity)
    weekly_workload['application_count'].sort_values(ascending=False).head(30).plot(
        kind='bar', ax=axes4[0, 0])
    axes4[0, 0].set_xlabel('Year-Week')
    axes4[0, 0].set_ylabel('Number of Applications')
    axes4[0, 0].set_title('Top 30 Weeks by Application Volume')
    axes4[0, 0].tick_params(axis='x', rotation=90)

    # Correlation between workload and processing time
    if len(weekly_workload) > 1:
        axes4[0, 1].scatter(weekly_workload['application_count'],
                           weekly_workload['processing_days'],
                           alpha=0.6)
        axes4[0, 1].set_xlabel('Weekly Application Volume')
        axes4[0, 1].set_ylabel('Average Processing Days')
        axes4[0, 1].set_title('Workload vs Processing Time Correlation')

        # Calculate correlation
        correlation = weekly_workload['application_count'].corr(weekly_workload['processing_days'])
        axes4[0, 1].annotate(f'Correlation: {correlation:.3f}',
                           xy=(0.05, 0.95), xycoords='axes fraction',
                           fontsize=10, backgroundcolor='white')

    # Analysis by NAICS_CODE (industry) if available
    if 'NAICS_CODE' in df.columns:
        industry_stats = df.groupby('NAICS_CODE')['processing_days'].agg(['mean', 'count']).sort_values('count', ascending=False).head(15)
        industry_stats['mean'].plot(kind='barh', ax=axes4[1, 0])
        axes4[1, 0].set_xlabel('Average Processing Days')
        axes4[1, 0].set_ylabel('NAICS Code (Industry)')
        axes4[1, 0].set_title('Processing Time by Industry (Top 15)')
        axes4[1, 0].invert_yaxis()

    # Analysis by job title if available
    if 'SOC_TITLE' in df.columns:
        job_stats = df.groupby('SOC_TITLE')['processing_days'].agg(['mean', 'count']).sort_values('count', ascending=False).head(10)
        job_stats['mean'].plot(kind='bar', ax=axes4[1, 1])
        axes4[1, 1].set_xlabel('Job Title')
        axes4[1, 1].set_ylabel('Average Processing Days')
        axes4[1, 1].set_title('Processing Time by Job Title (Top 10)')
        axes4[1, 1].tick_params(axis='x', rotation=45)

    plt.tight_layout()
    plt.show()


print("\n" + "="*60)
print("6. FEATURE IMPORTANCE ANALYSIS")
print("="*60)

# Prepare data for correlation analysis
eda_df = df.copy()

# Select key features for analysis
key_features = ['processing_days', 'VISA_CLASS', 'FULL_TIME_POSITION',
                'TOTAL_WORKER_POSITIONS', 'WAGE_RATE_OF_PAY_FROM', 'PREVAILING_WAGE']

available_features = [f for f in key_features if f in eda_df.columns]

# Convert categorical to numerical for correlation
eda_df_encoded = pd.get_dummies(eda_df[available_features].select_dtypes(include=['object']), drop_first=True)
numeric_features = eda_df[available_features].select_dtypes(include=[np.number])
eda_combined = pd.concat([numeric_features, eda_df_encoded], axis=1)

# Calculate correlation matrix
correlation_matrix = eda_combined.corr()
processing_corr = correlation_matrix['processing_days'].drop('processing_days').sort_values(ascending=False)

print("\nTop 10 Features Correlated with Processing Days:")
print(processing_corr.head(10))

print("\nBottom 10 Features Correlated with Processing Days:")
print(processing_corr.tail(10))

# Visualize feature correlations
fig5, axes5 = plt.subplots(1, 2, figsize=(16, 6))

# Bar plot of top correlations
top_corr = processing_corr.head(10)
colors = ['green' if x > 0 else 'red' for x in top_corr.values]
bars = axes5[0].barh(range(len(top_corr)), top_corr.values, color=colors)
axes5[0].set_yticks(range(len(top_corr)))
axes5[0].set_yticklabels(top_corr.index)
axes5[0].set_xlabel('Correlation Coefficient')
axes5[0].set_title('Top 10 Features Correlated with Processing Time')
axes5[0].invert_yaxis()

# Add correlation values
for i, (bar, val) in enumerate(zip(bars, top_corr.values)):
    axes5[0].text(0, bar.get_y() + bar.get_height()/2,
                 f'{val:.3f}',
                 va='center', ha='left' if val > 0 else 'right',
                 color='white' if abs(val) > 0.3 else 'black',
                 fontweight='bold')

# Heatmap of correlation matrix
im = axes5[1].imshow(correlation_matrix, cmap='coolwarm', aspect='auto',
                     vmin=-1, vmax=1)
axes5[1].set_title('Feature Correlation Heatmap')
axes5[1].set_xticks(range(len(correlation_matrix.columns)))
axes5[1].set_yticks(range(len(correlation_matrix.columns)))
axes5[1].set_xticklabels(correlation_matrix.columns, rotation=90, fontsize=8)
axes5[1].set_yticklabels(correlation_matrix.columns, fontsize=8)
plt.colorbar(im, ax=axes5[1], label='Correlation Coefficient')

plt.tight_layout()
plt.show()

print("\n" + "="*60)
print("7. KEY INSIGHTS AND SUMMARY")
print("="*60)

# Generate insights
insights = []

# Insight 1: Overall processing time
median_days = df['processing_days'].median()
mean_days = df['processing_days'].mean()
insights.append(f"1. Median processing time: {median_days:.1f} days")
insights.append(f"   Mean processing time: {mean_days:.1f} days")

# Insight 2: Seasonal patterns
if 'application_season' in df.columns:
    seasonal_avg = df.groupby('application_season')['processing_days'].mean()
    fastest_season = seasonal_avg.idxmin()
    slowest_season = seasonal_avg.idxmax()
    insights.append(f"2. Fastest processing season: {fastest_season} ({seasonal_avg[fastest_season]:.1f} days)")
    insights.append(f"   Slowest processing season: {slowest_season} ({seasonal_avg[slowest_season]:.1f} days)")

# Insight 3: Visa class patterns
if 'VISA_CLASS' in df.columns:
    visa_stats = df.groupby('VISA_CLASS')['processing_days'].agg(['mean', 'count']).sort_values('mean')
    fastest_visa = visa_stats.index[0]
    slowest_visa = visa_stats.index[-1]
    insights.append(f"3. Fastest visa type: {fastest_visa} ({visa_stats.loc[fastest_visa, 'mean']:.1f} days)")
    insights.append(f"   Slowest visa type: {slowest_visa} ({visa_stats.loc[slowest_visa, 'mean']:.1f} days)")

# Insight 4: Regional patterns
if 'EMPLOYER_STATE' in df.columns:
    state_stats = df.groupby('EMPLOYER_STATE')['processing_days'].agg(['mean', 'count']).sort_values('mean')
    if len(state_stats) > 5:
        fastest_states = state_stats.head(3).index.tolist()
        slowest_states = state_stats.tail(3).index.tolist()
        insights.append(f"4. Fastest processing states: {', '.join(fastest_states)}")
        insights.append(f"   Slowest processing states: {', '.join(slowest_states)}")

# Insight 5: Case status impact
if 'CASE_STATUS' in df.columns:
    status_impact = df.groupby('CASE_STATUS')['processing_days'].mean().sort_values()
    insights.append(f"5. Processing time varies significantly by case status:")
    for status, days in status_impact.items():
        insights.append(f"   - {status}: {days:.1f} days")

# Print insights
for insight in insights:
    print(insight)

# Save EDA results
eda_results = {
    'median_processing_days': median_days,
    'mean_processing_days': mean_days,
    'total_applications': len(df),
    'date_range': f"{df['RECEIVED_DATE'].min().date()} to {df['RECEIVED_DATE'].max().date()}",
    'top_correlations': processing_corr.head(10).to_dict()
}

print("\n" + "="*60)
print("EDA COMPLETED SUCCESSFULLY!")
print("="*60)
print(f"Key statistics saved for model development")
print(f"Visualizations generated: 5 comprehensive figures")