# -*- coding: utf-8 -*-
"""visa_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_YBuAPY1GCmBtQn1B2gkqQTmCACQxfRg
"""

# MODULE 3: Predictive Modeling
# AI Enabled Visa Status Prediction and Processing Time Estimator

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer

from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.tree import DecisionTreeRegressor
from xgboost import XGBRegressor

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.metrics import mean_absolute_percentage_error

plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")


# Load preprocessed data
print("\n1. Loading preprocessed data...")
file_path = '/content/drive/MyDrive/visa_data_preprocessed.csv'
df = pd.read_csv(file_path)

print(f"   Dataset shape: {df.shape}")
print(f"   Columns: {len(df.columns)}")

print("\n2. Dataset overview:")
print(f"   Date range: {df['RECEIVED_DATE'].min()} to {df['RECEIVED_DATE'].max()}")
print(f"   Total applications: {len(df):,}")


print("\n3. Target variable analysis (processing_days):")
print(f"   Mean: {df['processing_days'].mean():.2f} days")
print(f"   Median: {df['processing_days'].median():.2f} days")
print(f"   Std Dev: {df['processing_days'].std():.2f} days")
print(f"   Min: {df['processing_days'].min()} days")
print(f"   Max: {df['processing_days'].max()} days")
print(f"   Skewness: {df['processing_days'].skew():.3f}")


selected_features = [
    # Core features
    'VISA_CLASS',
    'CASE_STATUS',
    'FULL_TIME_POSITION',
    'EMPLOYER_STATE',
    'WORKSITE_STATE',

    # Temporal features (from Milestone 1)
    'application_year',
    'application_month',
    'application_season',
    'application_weekday',

    # Job-related features
    'JOB_TITLE',
    'SOC_TITLE',
    'TOTAL_WORKER_POSITIONS',

    # Wage-related features
    'WAGE_RATE_OF_PAY_FROM',
    'WAGE_UNIT_OF_PAY',
    'PREVAILING_WAGE',
    'PW_UNIT_OF_PAY',

    # Company features
    'NAICS_CODE',
    'H_1B_DEPENDENT',
    'WILLFUL_VIOLATOR',

    # Target variable
    'processing_days'
]

available_features = [f for f in selected_features if f in df.columns]
print(f"\nSelected {len(available_features)} features out of {len(selected_features)} proposed")

X = df[available_features].copy()
y = X.pop('processing_days')

print(f"\nFeature matrix shape: {X.shape}")
print(f"Target variable shape: {y.shape}")

print("\nFeature types:")
print(f"  Numerical features: {len(X.select_dtypes(include=[np.number]).columns)}")
print(f"  Categorical features: {len(X.select_dtypes(include=['object']).columns)}")



categorical_cols = X.select_dtypes(include=['object']).columns.tolist()
numerical_cols = X.select_dtypes(include=[np.number]).columns.tolist()

print(f"\nCategorical columns ({len(categorical_cols)}):")
for col in categorical_cols[:10]:
    print(f"  - {col}: {X[col].nunique()} unique values")
if len(categorical_cols) > 10:
    print(f"  ... and {len(categorical_cols) - 10} more")

print(f"\nNumerical columns ({len(numerical_cols)}):")
print("  " + ", ".join(numerical_cols))


print("\nHandling high cardinality categorical features...")
for col in categorical_cols:
    if X[col].nunique() > 50:
        print(f"  Reducing categories in '{col}' (had {X[col].nunique()} unique values)")

        top_categories = X[col].value_counts().head(20).index
        X[col] = X[col].apply(lambda x: x if x in top_categories else 'Other')

print("\nCreating preprocessing pipelines...")


numerical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])


categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
])

# Combine transformers
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_cols),
        ('cat', categorical_transformer, categorical_cols)
    ])




# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, shuffle=True
)

print(f"Training set: {X_train.shape[0]:,} samples")
print(f"Testing set: {X_test.shape[0]:,} samples")
print(f"Split ratio: 80% train, 20% test")



models = {
    'Linear Regression': LinearRegression(),
    'Ridge Regression': Ridge(alpha=1.0),
    'Lasso Regression': Lasso(alpha=1.0),
    'Decision Tree': DecisionTreeRegressor(random_state=42, max_depth=10),
    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),
    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),
    'XGBoost': XGBRegressor(n_estimators=100, random_state=42, n_jobs=-1, verbosity=0)
}

model_pipelines = {}
for name, model in models.items():
    model_pipelines[name] = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('regressor', model)
    ])


results = []
for name, pipeline in model_pipelines.items():
    print(f"\nTraining {name}...")

    start_time = datetime.now()
    pipeline.fit(X_train, y_train)
    train_time = (datetime.now() - start_time).total_seconds()

    y_pred_train = pipeline.predict(X_train)
    y_pred_test = pipeline.predict(X_test)

    train_mae = mean_absolute_error(y_train, y_pred_train)
    test_mae = mean_absolute_error(y_test, y_pred_test)

    train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))
    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))

    train_r2 = r2_score(y_train, y_pred_train)
    test_r2 = r2_score(y_test, y_pred_test)

    train_mape = mean_absolute_percentage_error(y_train, y_pred_train) * 100
    test_mape = mean_absolute_percentage_error(y_test, y_pred_test) * 100

    results.append({
        'Model': name,
        'Train_MAE': train_mae,
        'Test_MAE': test_mae,
        'Train_RMSE': train_rmse,
        'Test_RMSE': test_rmse,
        'Train_R2': train_r2,
        'Test_R2': test_r2,
        'Train_MAPE': train_mape,
        'Test_MAPE': test_mape,
        'Train_Time': train_time
    })

    print(f"  Training Time: {train_time:.2f} seconds")
    print(f"  MAE: Train={train_mae:.2f}, Test={test_mae:.2f} days")
    print(f"  RMSE: Train={train_rmse:.2f}, Test={test_rmse:.2f} days")
    print(f"  R² Score: Train={train_r2:.3f}, Test={test_r2:.3f}")
    print(f"  MAPE: Train={train_mape:.2f}%, Test={test_mape:.2f}%")

results_df = pd.DataFrame(results)
results_df = results_df.sort_values('Test_RMSE')


print("\nModels ranked by Test RMSE (lower is better):")
print(results_df[['Model', 'Test_RMSE', 'Test_MAE', 'Test_R2', 'Test_MAPE', 'Train_Time']].to_string(index=False))


fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# 1. RMSE Comparison
axes[0, 0].barh(results_df['Model'], results_df['Test_RMSE'], color='skyblue')
axes[0, 0].set_xlabel('RMSE (Days)')
axes[0, 0].set_title('Model Performance: Test RMSE Comparison')
axes[0, 0].axvline(results_df['Test_RMSE'].min(), color='red', linestyle='--', alpha=0.5)
for i, v in enumerate(results_df['Test_RMSE']):
    axes[0, 0].text(v + 0.5, i, f'{v:.1f}', va='center')

# 2. MAE Comparison
axes[0, 1].barh(results_df['Model'], results_df['Test_MAE'], color='lightgreen')
axes[0, 1].set_xlabel('MAE (Days)')
axes[0, 1].set_title('Model Performance: Test MAE Comparison')
axes[0, 1].axvline(results_df['Test_MAE'].min(), color='red', linestyle='--', alpha=0.5)
for i, v in enumerate(results_df['Test_MAE']):
    axes[0, 1].text(v + 0.5, i, f'{v:.1f}', va='center')

# 3. R² Score Comparison
axes[1, 0].barh(results_df['Model'], results_df['Test_R2'], color='lightcoral')
axes[1, 0].set_xlabel('R² Score')
axes[1, 0].set_title('Model Performance: Test R² Comparison')
axes[1, 0].axvline(results_df['Test_R2'].max(), color='red', linestyle='--', alpha=0.5)
for i, v in enumerate(results_df['Test_R2']):
    axes[1, 0].text(v + 0.01, i, f'{v:.3f}', va='center')

# 4. Training Time Comparison
axes[1, 1].barh(results_df['Model'], results_df['Train_Time'], color='gold')
axes[1, 1].set_xlabel('Training Time (Seconds)')
axes[1, 1].set_title('Model Training Time Comparison')
axes[1, 1].axvline(results_df['Train_Time'].min(), color='red', linestyle='--', alpha=0.5)
for i, v in enumerate(results_df['Train_Time']):
    axes[1, 1].text(v + 0.5, i, f'{v:.1f}', va='center')

plt.tight_layout()
plt.show()



best_model_name = results_df.iloc[0]['Model']
best_model = model_pipelines[best_model_name]

print(f"\n Best Performing Model: {best_model_name}")
print(f"   Test RMSE: {results_df.iloc[0]['Test_RMSE']:.2f} days")
print(f"   Test MAE: {results_df.iloc[0]['Test_MAE']:.2f} days")
print(f"   Test R²: {results_df.iloc[0]['Test_R2']:.3f}")
print(f"   Test MAPE: {results_df.iloc[0]['Test_MAPE']:.2f}%")


if best_model_name in ['Random Forest', 'Gradient Boosting', 'XGBoost', 'Decision Tree']:
    print(f"\n Feature Importance Analysis for {best_model_name}:")


    regressor = best_model.named_steps['regressor']

    preprocessor = best_model.named_steps['preprocessor']
    feature_names = []


    feature_names.extend(numerical_cols)


    cat_encoder = preprocessor.named_transformers_['cat'].named_steps['onehot']
    if hasattr(cat_encoder, 'get_feature_names_out'):
        cat_features = cat_encoder.get_feature_names_out(categorical_cols)
        feature_names.extend(cat_features)
    else:
        for col in categorical_cols:
            unique_vals = X_train[col].unique()
            for val in sorted(unique_vals):
                feature_names.append(f"{col}_{val}")


    if hasattr(regressor, 'feature_importances_'):
        importances = regressor.feature_importances_


        importance_df = pd.DataFrame({
            'Feature': feature_names[:len(importances)],
            'Importance': importances
        }).sort_values('Importance', ascending=False)

        print(f"\nTop 10 Most Important Features:")
        print(importance_df.head(10).to_string(index=False))


        fig, ax = plt.subplots(figsize=(12, 8))
        top_features = importance_df.head(15)
        ax.barh(range(len(top_features)), top_features['Importance'])
        ax.set_yticks(range(len(top_features)))
        ax.set_yticklabels(top_features['Feature'])
        ax.set_xlabel('Feature Importance Score')
        ax.set_title(f'Top 15 Feature Importances - {best_model_name}')
        ax.invert_yaxis()
        plt.tight_layout()
        plt.show()


        train_score = results_df[results_df['Model'] == best_model_name]['Train_R2'].values[0]
        test_score = results_df[results_df['Model'] == best_model_name]['Test_R2'].values[0]
        overfit_gap = train_score - test_score

        print(f"\n Overfitting Analysis:")
        print(f"   Train R²: {train_score:.3f}")
        print(f"   Test R²: {test_score:.3f}")
        print(f"   Gap: {overfit_gap:.3f}")

        if overfit_gap > 0.1:
            print(f"    Warning: Potential overfitting detected (gap > 0.1)")
        else:
            print(f"   Good generalization (gap ≤ 0.1)")



y_pred = best_model.predict(X_test)


fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# 1. Actual vs Predicted Scatter Plot
axes[0, 0].scatter(y_test, y_pred, alpha=0.5, s=10)
axes[0, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
axes[0, 0].set_xlabel('Actual Processing Days')
axes[0, 0].set_ylabel('Predicted Processing Days')
axes[0, 0].set_title(f'Actual vs Predicted - {best_model_name}')
axes[0, 0].grid(True, alpha=0.3)

# Add R² score to plot
axes[0, 0].text(0.05, 0.95, f'R² = {r2_score(y_test, y_pred):.3f}',
                transform=axes[0, 0].transAxes, fontsize=12,
                verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

# 2. Residual Plot
residuals = y_test - y_pred
axes[0, 1].scatter(y_pred, residuals, alpha=0.5, s=10)
axes[0, 1].axhline(y=0, color='r', linestyle='--')
axes[0, 1].set_xlabel('Predicted Processing Days')
axes[0, 1].set_ylabel('Residuals (Actual - Predicted)')
axes[0, 1].set_title(f'Residual Plot - {best_model_name}')
axes[0, 1].grid(True, alpha=0.3)

# 3. Error Distribution
axes[1, 0].hist(residuals, bins=50, edgecolor='black', alpha=0.7)
axes[1, 0].axvline(x=0, color='r', linestyle='--', linewidth=2)
axes[1, 0].set_xlabel('Prediction Error (Days)')
axes[1, 0].set_ylabel('Frequency')
axes[1, 0].set_title(f'Error Distribution - {best_model_name}')
axes[1, 0].grid(True, alpha=0.3)


mean_error = residuals.mean()
std_error = residuals.std()
axes[1, 0].text(0.05, 0.95, f'Mean Error: {mean_error:.2f} days\nStd Error: {std_error:.2f} days',
                transform=axes[1, 0].transAxes, fontsize=10,
                verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))


axes[1, 1].scatter(y_test, residuals, alpha=0.5, s=10)
axes[1, 1].axhline(y=0, color='r', linestyle='--')
axes[1, 1].set_xlabel('Actual Processing Days')
axes[1, 1].set_ylabel('Prediction Error (Days)')
axes[1, 1].set_title('Prediction Error vs Actual Value')
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()



test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))
test_r2 = r2_score(y_test, y_pred)
test_mae = mean_absolute_error(y_test, y_pred)

print(f"\n Final Model Performance on Test Set:")
print(f"   RMSE: {test_rmse:.2f} days")
print(f"   MAE: {test_mae:.2f} days")
print(f"   R²: {test_r2:.3f}")



import joblib
import pickle

# Save the model
model_filename = f'/content/drive/MyDrive/visa_processing_model_{best_model_name.replace(" ", "_")}.pkl'
joblib.dump(best_model, model_filename)

# Save the preprocessor separately
preprocessor_filename = '/content/drive/MyDrive/visa_preprocessor.pkl'
joblib.dump(preprocessor, preprocessor_filename)

# Save feature names
features_filename = '/content/drive/MyDrive/visa_features.pkl'
with open(features_filename, 'wb') as f:
    pickle.dump(available_features, f)

print(f"\n Model saved successfully!")
print(f"   Model file: {model_filename}")
print(f"   Preprocessor: {preprocessor_filename}")
print(f"   Features: {features_filename}")

summary = {
    'model_name': best_model_name,
    'test_rmse': float(test_rmse),
    'test_mae': float(test_mae),
    'test_r2': float(test_r2),
    'training_samples': len(X_train),
    'testing_samples': len(X_test),
    'features_used': available_features,
    'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
}

summary_filename = '/content/drive/MyDrive/model_summary.json'
import json
with open(summary_filename, 'w') as f:
    json.dump(summary, f, indent=2)

print(f"   Summary: {summary_filename}")


print(f"\n Business Impact Analysis:")
print(f"   Mean Processing Time: {y.mean():.1f} days")
print(f"   Model MAE: {test_mae:.1f} days")
print(f"   Accuracy: {(1 - (test_mae / y.mean())) * 100:.1f}%")

if 'VISA_CLASS' in X_test.columns:
    print(f"\n Error Analysis by Visa Class:")


    error_df = pd.DataFrame({
        'Actual': y_test.values,
        'Predicted': y_pred,
        'VISA_CLASS': X_test['VISA_CLASS'].values
    })
    error_df['Error'] = abs(error_df['Actual'] - error_df['Predicted'])


    visa_errors = error_df.groupby('VISA_CLASS').agg({
        'Error': ['mean', 'std', 'count']
    }).round(2)

    print(visa_errors.head(10))